import whisperx
import gc
import torch
from pydub import AudioSegment
from TTS.api import TTS
from googletrans import Translator
print(torch.cuda.is_available())  # to check if cuda available or not; will return True if available
model = whisperx.load_model("large-v3", "cuda", compute_type="float16")# TO LOAD YOUR VERSION MODEL FROM WHISPERX
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)
language={
            "english" :'en',
            "spanish" :'es',
            "french" :'fr',
            "german": 'de',
            "italian" :'it',
            "portuguese":'pt',
            "polish": 'pl',
            "turkish" :'tr',
            "russian" :"ru",
            "dutch": "nl",
            "czech": "cs",
            "arabic" :'ar',
            "chinese": 'cn',
            "japanese" :"ja",
            "hungarian": 'hu',
            "korean" :'ko',
            "hindi": 'hi'
        }


def check_lang(input_lang):
    lang = None  # Default value
    for key, value in language.items():
        if input_lang.lower() == key:
            lang = value
            break
    return lang


def WhisperX(audio_file):# TO TRANSCRIBE AUDIO FILE
    batch_size = 4
    audio = whisperx.load_audio(audio_file)
    result = model.transcribe(audio, batch_size=batch_size)

    model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
    result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)
    # language_code=result["language"]

    return result, audio


def diarizer(result,audio,numper_of_speaker,target_lang):
    diarize_model = whisperx.DiarizationPipeline(use_auth_token="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",device=device)
    diarize_segments = diarize_model(audio, min_speakers=numper_of_speaker, max_speakers=numper_of_speaker)
    result2 = whisperx.assign_word_speakers(diarize_segments, result) 
    unique_speakers_array=diarize_segments.speaker.unique()
    speaker_name = unique_speakers_array.tolist()

    #speaker_text_pairs = [(segment["speaker"], segment["text"]) for segment in result["segments"]]
    # diar = diarize_segments[['start', 'end', 'label', 'speaker']].values.tolist()
    words=result2['word_segments']
    print(words)
    return words,speaker_name



def collect_sentences(words):
    sentences = []
    current_sentence = {'speaker': None, 'words': []}

    for segment in words:
        speaker = segment.get('speaker')  # Use .get() to safely retrieve the speaker value
        if current_sentence['speaker'] is None:
            current_sentence['speaker'] = speaker

        elif speaker != current_sentence['speaker']:
            sentences.append(
                (current_sentence['speaker'], ' '.join(current_sentence['words']))
            )
            current_sentence = {'speaker': speaker, 'words': []}
        current_sentence['words'].append(segment['word'])

    # Append the last sentence
    if current_sentence['words']:
        sentences.append(
            (current_sentence['speaker'], ' '.join(current_sentence['words']))
        )
    print(sentences)
    return sentences

# Using the provided data

def translate_text(text, target_lang):
    translator = Translator()
    translated_text = translator.translate(text, dest=target_lang)
    return translated_text.text

def collect_sentences_with_translation(words, target_lang):
    translated_sentences = []

    for segment in words:
        speaker = segment[0]
        sentence = segment[1]
        translated_sentence = translate_text(sentence, target_lang)
        translated_sentences.append((speaker, translated_sentence))

    return translated_sentences


def get_speaker_input(speaker_name):
    num_users=len(speaker_name)
    speakers = {}
    for i in range(num_users):
        speaker_num = str(i+1-1).zfill(2)  # Convert to 2-digit string
        speaker_name = f"SPEAKER_{speaker_num}"
        user_input = input(f"Enter input for {speaker_name}: ")
        speakers[speaker_name] = user_input
        i+=1
    return speakers


def multi_user(speaker_text_pairs,speakers,target_lang):
    combined_audio = AudioSegment.silent(duration=0)
    data=speaker_text_pairs
    speakers=speakers
    for speaker, text in data:
        print(text)
        print(speaker)
        tts.tts_to_file(text=text, speaker_wav=speakers[speaker], language=target_lang, file_path="output.wav")
        audio = AudioSegment.from_file("output.wav")
        combined_audio += audio
    All_Audio=combined_audio.export("combined_output.wav", format="wav")
    return All_Audio


def all(audio_file,numper_of_speaker,target_lang):
    lang=check_lang(target_lang)
    result, audio=WhisperX(audio_file)
    words,speaker_name=diarizer(result,audio,numper_of_speaker,lang)
    sentences=collect_sentences(words)
    speaker_text_pairs=collect_sentences_with_translation(sentences, lang)
    speakers=get_speaker_input(speaker_name)
    All_Audio=multi_user(speaker_text_pairs,speakers,lang)
    return All_Audio


audio_file = "/content/hanady_mostafa.wav"
numper_of_speaker=2
target_lang="english"
x=all(audio_file,numper_of_speaker,target_lang)
print(x)
